{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embedding 모델\n",
    "model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def embed_documents(self, documents):\n",
    "        return model.encode(documents)\n",
    "\n",
    "text_splitter = SemanticChunker(CustomEmbeddings())\n",
    "\n",
    "output_file = '/data/ephemeral/home/data/document_chunk.jsonl'\n",
    "\n",
    "with open('/data/ephemeral/home/data/documents.jsonl', 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "\n",
    "        data = json.loads(line)\n",
    "        docid = data['docid']\n",
    "        src = data['src']\n",
    "        content = data['content']\n",
    "\n",
    "        chunks = text_splitter.split_text(content)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            output_data = {\n",
    "                \"docid\": docid,\n",
    "                \"src\": src,\n",
    "                \"content\": chunk\n",
    "            }\n",
    "\n",
    "            outfile.write(json.dumps(output_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"청킹 완료 :'{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘라스틱서치의 데몬 인스턴스 만들기\n",
    "# Nori 설치 이전에 데몬을 생성하면 Nori가 바로 사용할 수 없음, 이때는 데몬 재실행 필요\n",
    "\n",
    "import os\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "es_server = Popen(['/data/ephemeral/home/elasticsearch-8.8.0/bin/elasticsearch'],\n",
    "                  stdout=PIPE, stderr=STDOUT,\n",
    "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                 )\n",
    "\n",
    "# 인스턴스를 로드하는 데 약간의 시간이 걸림\n",
    "import time\n",
    "time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데몬이 구동되었는지 확인 (세개의 daemon process가 있어야 함)\n",
    "!ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'elastic'\n",
    "\n",
    "# 위 명령 실행 결과의 마지막 부분인 PASSWORD elastic 값으로 교체 필요\n",
    "password = 'PASSWORD elastic'\n",
    "\n",
    "es = Elasticsearch(['https://localhost:9200'], basic_auth=(username, password), ca_certs=\"/data/ephemeral/home/elasticsearch-8.8.0/config/certs/http_ca.crt\")\n",
    "\n",
    "resp = dict(es.info())\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer 모델 초기화\n",
    "# model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# SetntenceTransformer를 이용하여 임베딩 생성\n",
    "def get_embedding(sentences):\n",
    "    return model.encode(sentences)\n",
    "\n",
    "\n",
    "# 주어진 문서의 리스트에서 배치 단위로 임베딩 생성\n",
    "def get_embeddings_in_batches(docs, batch_size=100):\n",
    "    batch_embeddings = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        contents = [doc[\"content\"] for doc in batch]\n",
    "        embeddings = get_embedding(contents)\n",
    "        batch_embeddings.extend(embeddings)\n",
    "        print(f'batch {i}')\n",
    "    return batch_embeddings\n",
    "\n",
    "\n",
    "# 새로운 index 생성\n",
    "def create_es_index(index, settings, mappings):\n",
    "    # 인덱스가 이미 존재하는지 확인\n",
    "    if es.indices.exists(index=index):\n",
    "        # 인덱스가 이미 존재하면 설정을 새로운 것으로 갱신하기 위해 삭제\n",
    "        es.indices.delete(index=index)\n",
    "    # 지정된 설정으로 새로운 인덱스 생성\n",
    "    es.indices.create(index=index, settings=settings, mappings=mappings)\n",
    "\n",
    "\n",
    "# 지정된 인덱스 삭제\n",
    "def delete_es_index(index):\n",
    "    es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# Elasticsearch 헬퍼 함수를 사용하여 대량 인덱싱 수행\n",
    "def bulk_add(index, docs):\n",
    "    # 대량 인덱싱 작업을 준비\n",
    "    actions = [\n",
    "        {\n",
    "            '_index': index,\n",
    "            '_source': doc\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return helpers.bulk(es, actions)\n",
    "\n",
    "\n",
    "# 역색인을 이용한 검색\n",
    "def sparse_retrieve(query_str, size):\n",
    "    query = {\n",
    "        \"match\": {\n",
    "            \"content\": {\n",
    "                \"query\": query_str\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es.search(index=\"test\", query=query, size=size, sort=\"_score\")\n",
    "\n",
    "\n",
    "# Vector 유사도를 이용한 검색\n",
    "def dense_retrieve(query_str, size):\n",
    "    # 벡터 유사도 검색에 사용할 쿼리 임베딩 가져오기\n",
    "    query_embedding = get_embedding([query_str])[0]\n",
    "\n",
    "    # KNN을 사용한 벡터 유사성 검색을 위한 매개변수 설정\n",
    "    knn = {\n",
    "        \"field\": \"embeddings\",\n",
    "        \"query_vector\": query_embedding.tolist(),\n",
    "        \"k\": size,\n",
    "        \"num_candidates\": 100\n",
    "    }\n",
    "\n",
    "    # 지정된 인덱스에서 벡터 유사도 검색 수행\n",
    "    return es.search(index=\"test\", knn=knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역색인 + Vector 유사도 혼합\n",
    "def hybrid_retrieve(query_str, size):\n",
    "    # 벡터 유사도 검색에 사용할 쿼리 임베딩 가져오기\n",
    "    query_embedding = get_embedding([query_str])[0]\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query_str,\n",
    "                    # \"boost\": 0.0030\n",
    "                    \"boost\" : 0.0025 \n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"knn\": {\n",
    "            \"field\": \"embeddings\",\n",
    "            \"query_vector\": query_embedding.tolist(),\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 50,\n",
    "            \"boost\": 1\n",
    "        },\n",
    "        \"collapse\":{\n",
    "            \"field\": \"docid.keyword\",  # keyword 타입으로 설정된 필드 사용\n",
    "            \"inner_hits\": {\n",
    "                \"name\": \"top_hits\",\n",
    "                \"size\": 1,\n",
    "                \"sort\": [\n",
    "                    {\"_score\": \"desc\"}\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "        \"size\": size\n",
    "    }\n",
    "    # 지정된 인덱스에서 벡터 유사도 검색 수행\n",
    "    return es.search(index=\"test\", body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 색인을 위한 setting 설정\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"nori\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"nori_tokenizer\",\n",
    "                \"decompound_mode\": \"mixed\",\n",
    "                \"filter\": [\"nori_posfilter\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"nori_posfilter\": {\n",
    "                \"type\": \"nori_part_of_speech\",\n",
    "                # 어미, 조사, 구분자, 줄임표, 지정사, 보조 용언 등\n",
    "                \"stoptags\": [\"E\", \"J\", \"SC\", \"SE\", \"SF\", \"VCN\", \"VCP\", \"VX\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 색인을 위한 mapping 설정 (역색인 필드, 임베딩 필드 모두 설정)\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        # 역색인 필드 \n",
    "        \"content\": {\"type\": \"text\", \"analyzer\": \"nori\"}, \n",
    "        # 임베딩 필드 \n",
    "        \"embeddings\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_es_index(\"test\", settings, mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Chunking content 필드에 대한 임베딩 생성\n",
    "index_docs = []\n",
    "with open(\"../data/document_chunk.jsonl\") as f:\n",
    "#with open(\"/data/ephemeral/home/data/merged_documents.jsonl\") as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "embeddings = get_embeddings_in_batches(docs)\n",
    "\n",
    "\"\"\" \n",
    "docs[0] -> \n",
    "{'docid': '42508ee0-c543-4338-878e-d98c6babee66',\n",
    " 'src': 'ko_mmlu__nutrition__test',\n",
    " 'content': '건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다. ....\n",
    " }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = docs[0], len(embedding) = 768\n",
    "# doc_key = docid, src, content\n",
    "for doc, embedding in zip(docs, embeddings):  \n",
    "    doc['embeddings'] = embedding.tolist() \n",
    "    index_docs.append(doc)\n",
    "    \n",
    "# index_docs_key = docid, src, content, embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = bulk_add(\"test\", index_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"네트웍 계층 중 IP에 대해 설명해줘.\"\n",
    "# 역색인을 사용하는 검색 예제\n",
    "search_result_retrieve = hybrid_retrieve(test_query, 10)\n",
    "\n",
    "# 결과 출력 테스트\n",
    "for rst in search_result_retrieve['hits']['hits']:\n",
    "    print('score:', rst['_score'], 'docid: ',rst[\"_source\"][\"docid\"], 'source:', rst['_source'][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG를 구현\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "\n",
    "# OpenAI API 키를 환경변수에 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "\n",
    "client = OpenAI()\n",
    "# 사용할 모델\n",
    "llm_model = \"gpt-4o\"\n",
    "\n",
    "# RAG 구현에 필요한 Question Answering을 위한 LLM  프롬프트\n",
    "persona_qa = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instructions\n",
    "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
    "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
    "- 한국어로 답변을 생성한다.\n",
    "\"\"\"\n",
    "\n",
    "# RAG 구현에 필요한 질의 분석 및 검색 이외의 일반 질의 대응을 위한 LLM 프롬프트\n",
    "persona_function_calling = \"\"\"\n",
    "## Role: 과학 상식 전문가\n",
    "\n",
    "## Instruction\n",
    "- 사용자가 대화를 통해 지식에 관한 주제로 질문하면 반드시 search api를 호출할 수 있어야 한다.\n",
    "- 지식과 관련되지 않은 나머지 대화 메시지에는 함수 호출 없이 적절한 대답을 생성한다.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling에 사용할 함수 정의\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"search relevant documents\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"long_question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        # \"description\": \"Final query suitable for use in search from the user messages history\"\n",
    "                        # \"description\": \"Keywords suitable for use in search engine.\"\n",
    "                        # \"description\": \"User's question in Korean, including all the keywords in the user messages\"\n",
    "                        \"description\": \"User's question in Korean. Full message if the user message is single-turn.\"}\n",
    "                        #\"description\" : \"User's question in Korean. Full message if the user message is single-turn. Additionally, categorize the question and infer user's intent.\"}\n",
    "                },\n",
    "                \"required\": [\"long_question\"],\n",
    "                \"type\": \"object\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Ranker 모델 로드\n",
    "model_path = 'Dongjin-kr/ko-reranker'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_reranker = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model_reranker.eval()\n",
    "\n",
    "def exp_normalize(x):\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 \n",
    "\n",
    "def rerank_with_model(query, retrieved_docs):\n",
    "    # Query와 검색된 문서를 쌍으로 만듬\n",
    "    pairs = [[query, doc[\"_source\"][\"content\"]] for doc in retrieved_docs]\n",
    "    \n",
    "    # Ranker 모델을 통해 점수 계산\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = model_reranker(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        normalized_scores = exp_normalize(scores.numpy())\n",
    "\n",
    "    # 점수에 따라 문서를 재정렬하고 상위 5개의 문서만 남김\n",
    "    reranked_docs = sorted(zip(retrieved_docs, normalized_scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM과 검색엔진을 활용한 RAG 구현\n",
    "def answer_question(messages):\n",
    "    # 함수 출력 초기화\n",
    "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
    "\n",
    "    # 질의 분석 및 검색 이외의 질의 대응을 위한 LLM 활용\n",
    "    msg = [{\"role\": \"system\", \"content\": persona_function_calling}] + messages\n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=msg,\n",
    "            tools=tools,\n",
    "            #tool_choice={\"type\": \"function\", \"function\": {\"name\": \"search\"}},\n",
    "            temperature=0,\n",
    "            seed=1,\n",
    "            timeout=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return response\n",
    "\n",
    "    # 검색이 필요한 경우 검색 호출 후 결과를 활용하여 답변 생성\n",
    "    if result.choices[0].message.tool_calls:\n",
    "        tool_call = result.choices[0].message.tool_calls[0]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        standalone_query = function_args.get(\"long_question\")\n",
    "\n",
    "        # 검색 결과 추출\n",
    "        # search_result = hybrid_retrieve(standalone_query, 100)\n",
    "        search_result = hybrid_retrieve(standalone_query, 10)\n",
    "        # 검색 결과를 Ranker 모델로 재정렬\n",
    "        reranked_docs = rerank_with_model(standalone_query, search_result['hits']['hits'])\n",
    "\n",
    "        response[\"standalone_query\"] = standalone_query\n",
    "        retrieved_context = []\n",
    "        \n",
    "        # 중복을 방지하기 위한 set\n",
    "        #seen_docids = set()\n",
    "        \n",
    "        for doc, score in reranked_docs:\n",
    "            #docid = doc[\"_source\"][\"docid\"]\n",
    "            # docid가 이미 추가된 경우는 생략\n",
    "            #if docid not in seen_docids:\n",
    "            retrieved_context.append(doc[\"_source\"][\"content\"])\n",
    "            response[\"topk\"].append(doc[\"_source\"][\"docid\"])\n",
    "            response[\"references\"].append({\"score\": float(score), \"content\": doc[\"_source\"][\"content\"]})\n",
    "                #seen_docids.add(docid)  # 이미 처리한 docid를 set에 추가\n",
    "                \n",
    "        # 재정렬된 문서들을 LLM에 다시 넣어 답변 생성\n",
    "        # messages.append({\"role\": \"assistant\", \"content\": json.dumps(retrieved_context)})\n",
    "        # msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages\n",
    "        # try:\n",
    "        #     qaresult = client.chat.completions.create(\n",
    "        #             model=llm_model,\n",
    "        #             messages=msg,\n",
    "        #             temperature=0,\n",
    "        #             seed=1,\n",
    "        #             timeout=30\n",
    "        #         )\n",
    "        # except Exception as e:\n",
    "        #     traceback.print_exc()\n",
    "        #     return response\n",
    "        # response[\"answer\"] = qaresult.choices[0].message.content\n",
    "\n",
    "    # 검색이 필요하지 않은 경우 바로 답변 생성\n",
    "    else:\n",
    "        response[\"answer\"] = result.choices[0].message.content\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출후 파일에 저장\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            response = answer_question(j[\"msg\"])\n",
    "            print(f'Answer: {response[\"answer\"]}\\n')\n",
    "\n",
    "            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n",
    "            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n",
    "            \n",
    "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
    "            idx += 1\n",
    "\n",
    "# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n",
    "eval_rag(\"../data/eval.jsonl\", \"../submission/[16.3]submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
